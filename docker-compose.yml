
version: '3.8'

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - ./data:/app/data
    environment:
      - DATABASE_URL=sqlite+aiosqlite:///./data/cobol_whisperer.db
      - USE_LLM_SERVICE=true
      - LLM_SERVICE_URL=http://llm-service:8080
      - DEBUG=true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - llm-service

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    ports:
      - "5173:5173"
    volumes:
      - ./src:/app/src
      - ./public:/app/public
    environment:
      - VITE_API_URL=http://localhost:8000
    restart: unless-stopped
    depends_on:
      - backend

  llm-service:
    build:
      context: ./backend
      dockerfile: Dockerfile.llm
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models
      - ./data/fine_tuned:/app/fine_tuned
    environment:
      - PORT=8080
      - HOST=0.0.0.0
      - USE_LLAMA_CPP=true
      - LLAMA_MODEL_PATH=/app/models/llama-3-8b-instruct.gguf
      # Optional: Set to automatically download a model if not present
      # - MODEL_DOWNLOAD_URL=https://huggingface.co/TheBloke/Llama-3-8B-Instruct-GGUF/resolve/main/llama-3-8b-instruct.Q5_K_M.gguf
      # Optional: Fallback to HF model if no GGUF model is found
      # - DEFAULT_HF_MODEL=meta-llama/Llama-3-8B-Instruct
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  data:
